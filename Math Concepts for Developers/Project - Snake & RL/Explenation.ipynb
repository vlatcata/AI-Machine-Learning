{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77f4f0a",
   "metadata": {},
   "source": [
    "Deep Q Learning model - Based on a Neural Network\n",
    "\n",
    "We use gradient descent to match the predicted Q-values to the result of those actions.\n",
    "The closer we get to 0, the better the model is performing.\n",
    "The goal is for the prediction to always match the result. That is achieved by getting the global minimum.\n",
    "\n",
    "$ Q(s, a) $ - *Action value function*\n",
    "\n",
    "Or said simply... *Expected cumulative reward from state **s** by taking action **a***.\n",
    "\n",
    "Where:\n",
    "\n",
    "Q = Quality of action\n",
    "\n",
    "s = state\n",
    "\n",
    "a = action\n",
    "\n",
    "$ Q^target(s, a) = R + \\gamma.((max(Q(s', a'))) / 2) $\n",
    "\n",
    "\n",
    "Each move the model outputs 3 possible actions.\n",
    "Then the agent picks the one with the highest Q-value.\n",
    "\n",
    "State - A vector of many bool values\n",
    "\n",
    "**Bellman Equation** for Double Q-Learning: $$ NewQ(s, a) = Q(s, a) + \\alpha[R(s, a) + \\gamma maxQ'(s', a') - Q(s, a)] $$\n",
    "\n",
    "We modify the base equation to reduce overestimation.\n",
    "\n",
    "Our loss function is $$(Q_{new} - Q)^2$$\n",
    "\n",
    "\n",
    "In pygame the coordinate system works differently... instead of y growing by going up vertically,([0, 0] is not in the middle of the screen, but it's on the top left edge) it grows down, this is done to mimic the pixels in our monitors.\n",
    "\n",
    "$ \\gamma $ or **Gamma** controls how much the agent values future rewards compared to immediate rewards.\n",
    "\n",
    "A **tensor** is a generalized data type used by reinforcement learning models _(such as Pytorch)_ to represent and process all kinds of numerical data, such as states, actions, and rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a5595",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "https://www.youtube.com/watch?v=PJl4iabBEz0&list=PLfR10wejCzo_OL-6OsBV-4jAPnSncvZZH\n",
    "\n",
    "https://en.wikipedia.org/wiki/Reinforcement_learning\n",
    "\n",
    "https://www.geeksforgeeks.org/what-is-reinforcement-learning/\n",
    "\n",
    "https://www.geeksforgeeks.org/snake-game-in-python-using-pygame-module/\n",
    "\n",
    "https://pytorch.org/\n",
    "\n",
    "https://www.pygame.org/docs/\n",
    "\n",
    "https://docs.pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "\n",
    "\n",
    "And of course... **ChatGPT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ae516",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "Maybe move the BLOCK_SIZE and SPEED of game.py in the Snake class so we can use it in the agent directly.\n",
    "\n",
    "Maybe even inject it from the controller so we can dynamically change it in the future... not sure tho.\n",
    "\n",
    "Optimize the model by rewarding it for moving towards the food, and taking rewards away for moving away to prevent endless loops.\n",
    "Keep positions/states of the whole body so it doesn't trap itself. Or think of some way to deal with that...\n",
    "\n",
    "Finish the ipynb file for the project, explain everything and so on...\n",
    "\n",
    "Implement saving and importing the trained model.\n",
    "\n",
    "Make display bigger... maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab1ccca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNAKE_GAME_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
