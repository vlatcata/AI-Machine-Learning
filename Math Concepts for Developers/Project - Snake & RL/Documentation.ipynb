{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5dbab8",
   "metadata": {},
   "source": [
    "# Snake game with AI\n",
    "\n",
    "### This project is about making a snake game and using Reinforcement Learning to train a model to play\n",
    "\n",
    "**For the random people who find this in github, the best practice to run it is to create a venv and install all packages from the requirements.txt.**\n",
    "To do that, run:\n",
    "\n",
    "```\n",
    "python -m venv gameenv\n",
    "gameenv\\Scripts\\activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "So... In order to make an AI model to play the game, first we need to code up the actual game.\n",
    "\n",
    "For the game, the libraries we will need are:\n",
    "```python\n",
    "import pygame\n",
    "import random\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "The render font that I chose for the UI is arial.ttf, so we need to download that into the game folder.\n",
    "\n",
    "Setting up the game we have the font, an enum class for Directions, a namedtuple we will be using to define each point in the game window, some color constants, block size we will be using for the objects in the game (snake body and food) and speed - which is the framerate.\n",
    "\n",
    "**Helpers we have for the game:**\n",
    "1. `arial.ttf` – font for game text\n",
    "2. `Direction` (Enum) – movement directions\n",
    "3. `Point` – named tuple for clean point management\n",
    "4. Color constants – `WHITE`, `RED`, etc. for `pygame`\n",
    "5. `BLOCK_SIZE` – the size of a single block (snake body / food)\n",
    "6. `SPEED` – frames per second (FPS) of the game\n",
    "\n",
    "**Game methods:**\n",
    "1. `__init__` - the constructor, in which we specialize the display width and height\n",
    "2. `_place_food()` - we place the food on a random place.\n",
    "3. `play_step()` - we change the snake's direction based on user input\n",
    "4. `_is_collision()` - we check if the snake bumped into something\n",
    "5. `_move()` - we apply the user input direction\n",
    "6. `_update_ui()` - we display and update the UI\n",
    "\n",
    "### How the game works\n",
    "\n",
    "First, we start by placing the snake horizontally and placing the food on a random place. The snake's head is at the center of the screen with it's body going to the left by the _x_ axis, and the default starting movement being right _relative to the screen_.\n",
    "\n",
    "Then we start moving. The way we simulate movement is simple. We don't move each part of the snake each frame, instead, every step, we place a new head in the direction that the player specified and remove the tail if there was not food at that place. If there is food there, we don't remov the head and continue the loop. When we repeat that process really fast it looks like the whole snake is moving, but instead... we simply add a head and cut the tail really fast.\n",
    "\n",
    "**Note - movement:** Game libraries don't use a normal coordinate system like in math. In games, in order to simulate monitor pixels, _x_ grows to the right, but _y_ grows downwards, not upwards.\n",
    "\n",
    "Each step/frame we check if there is a collision and stop the game if there is.\n",
    "\n",
    "And of course we update the UI each frame too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c93397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "\n",
    "pygame.init()\n",
    "font = pygame.font.Font('arial.ttf', 25)\n",
    "\n",
    "class Direction(Enum):\n",
    "    RIGHT = 1\n",
    "    LEFT = 2\n",
    "    UP = 3\n",
    "    DOWN = 4\n",
    "    \n",
    "Point = namedtuple('Point', 'x, y')\n",
    "\n",
    "# rgb colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (200,0,0)\n",
    "BLUE1 = (0, 0, 255)\n",
    "BLUE2 = (0, 100, 255)\n",
    "BLACK = (0,0,0)\n",
    "\n",
    "BLOCK_SIZE = 20\n",
    "SPEED = 10\n",
    "\n",
    "class SnakeGame:\n",
    "    \n",
    "    def __init__(self, w=640, h=480):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        # init display\n",
    "        self.display = pygame.display.set_mode((self.w, self.h))\n",
    "        pygame.display.set_caption('Snake')\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "        # init game state\n",
    "        self.direction = Direction.RIGHT\n",
    "        \n",
    "        self.head = Point(self.w/2, self.h/2)\n",
    "        self.snake = [self.head, \n",
    "                      Point(self.head.x-BLOCK_SIZE, self.head.y),\n",
    "                      Point(self.head.x-(2*BLOCK_SIZE), self.head.y)]\n",
    "        \n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self._place_food()\n",
    "        \n",
    "    def _place_food(self):\n",
    "        x = random.randint(0, (self.w-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE \n",
    "        y = random.randint(0, (self.h-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n",
    "        self.food = Point(x, y)\n",
    "        if self.food in self.snake:\n",
    "            self._place_food()\n",
    "        \n",
    "    def play_step(self):\n",
    "        # 1. collect user input\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_LEFT:\n",
    "                    self.direction = Direction.LEFT\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    self.direction = Direction.RIGHT\n",
    "                elif event.key == pygame.K_UP:\n",
    "                    self.direction = Direction.UP\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    self.direction = Direction.DOWN\n",
    "        \n",
    "        # 2. move\n",
    "        self._move(self.direction) # update the head\n",
    "        self.snake.insert(0, self.head)\n",
    "        \n",
    "        # 3. check if game over\n",
    "        game_over = False\n",
    "        if self._is_collision():\n",
    "            game_over = True\n",
    "            return game_over, self.score\n",
    "            \n",
    "        # 4. place new food or just move\n",
    "        if self.head == self.food:\n",
    "            self.score += 1\n",
    "            self._place_food()\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "        \n",
    "        # 5. update ui and clock\n",
    "        self._update_ui()\n",
    "        self.clock.tick(SPEED)\n",
    "        # 6. return game over and score\n",
    "        return game_over, self.score\n",
    "    \n",
    "    def _is_collision(self):\n",
    "        # hits boundary\n",
    "        if self.head.x > self.w - BLOCK_SIZE or self.head.x < 0 or self.head.y > self.h - BLOCK_SIZE or self.head.y < 0:\n",
    "            return True\n",
    "        # hits itself\n",
    "        if self.head in self.snake[1:]:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    def _update_ui(self):\n",
    "        self.display.fill(BLACK)\n",
    "        \n",
    "        for pt in self.snake:\n",
    "            pygame.draw.rect(self.display, BLUE1, pygame.Rect(pt.x, pt.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "            pygame.draw.rect(self.display, BLUE2, pygame.Rect(pt.x+4, pt.y+4, 12, 12))\n",
    "            \n",
    "        pygame.draw.rect(self.display, RED, pygame.Rect(self.food.x, self.food.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "        \n",
    "        text = font.render(\"Score: \" + str(self.score), True, WHITE)\n",
    "        self.display.blit(text, [0, 0])\n",
    "        pygame.display.flip()\n",
    "        \n",
    "    def _move(self, direction):\n",
    "        x = self.head.x\n",
    "        y = self.head.y\n",
    "        if direction == Direction.RIGHT:\n",
    "            x += BLOCK_SIZE\n",
    "        elif direction == Direction.LEFT:\n",
    "            x -= BLOCK_SIZE\n",
    "        elif direction == Direction.DOWN:\n",
    "            y += BLOCK_SIZE\n",
    "        elif direction == Direction.UP:\n",
    "            y -= BLOCK_SIZE\n",
    "            \n",
    "        self.head = Point(x, y)\n",
    "\n",
    "game = SnakeGame()\n",
    "    \n",
    "# game loop\n",
    "while True:\n",
    "    game_over, score = game.play_step()\n",
    "        \n",
    "    if game_over == True:\n",
    "        break\n",
    "        \n",
    "print('Final Score', score)\n",
    "\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf992b",
   "metadata": {},
   "source": [
    "\n",
    "### Next Steps: Hooking up an AI\n",
    "\n",
    "Now that the game mechanics are coded, the next step is to build an agent that plays the game using **Reubfircement Learning (RL)**\n",
    "\n",
    "In order for that to happen, first we need to extend the `play_step()` and `move()` methods, and then we need to remove the `__name__ == __main__` check at the end as we won't be starting up the game directly anymore.\n",
    "\n",
    "#### Modifying the `play_step()`:\n",
    "\n",
    "This is where the **Reinforcement Learning** magic happens.\n",
    "\n",
    "Reinforcement learning is simple at its core. During training, we define rewards and penalties. The model is rewarded for good actions and penalized for bad ones. It learns through feedback:\n",
    "\n",
    "``I ran into a wall and got a -10 reward --- I shouldn't do that.``\n",
    "\n",
    "``I ate the food and got +10 --- eating food is good!``\n",
    "\n",
    "Since the agent will now decide the snake’s direction, we add a second parameter to ``play_step()``: an ``action``, represented as a vector of type ``[float, float, float]`` (more on that later). This replaces keyboard input — we no longer listen to ``KEYDOWN`` events from the user.\n",
    "\n",
    "Now... back to the sauce. We calculate the reward based on a couple of things. The main ones are obviously food and death. We reward the model 10 points for eating foor and -10 for dying.\n",
    "But as you can guess, since the display width is 640p, height is 480p and block size is 20x20, that means the probability of the snake getting the food is: $$\\frac{640}{20} \\times \\frac{480}{20} = 32 \\times 24 = 768 \\text{ total cells}$$ Meaning each move our change of getting food is: $$ \\frac{1}{768} \\approx 0.0013 \\text{ or } 0.13\\% $$\n",
    "\n",
    "#### So what do we do?\n",
    "\n",
    "**Well... we simply need to calculate the reward better and more precisely. We introduce _immediate rewards_:**\n",
    "1. Since the agent will now decide the snake’s direction, we add a second parameter to play_step(): an action, represented as a vector of type [float, float, float] (more on that later). This replaces keyboard input — we no longer listen to KEYDOWN events from the user.\n",
    "    1. If the new distance is shorter, we give **+1 reward**.\n",
    "    2. If it’s **greater**, we apply a **−0.5** penalty.\n",
    "2. We check for body parts in 8 surrounding directions (excluding the neck).\n",
    "    1. For each nearby segment, we apply **−0.1**.\n",
    "    2. This encourages the snake to avoid trapping itself.\n",
    "3. If the snake loops around for too long, we penalize it using:\n",
    "    $$ Penalty = 100 * len(snake) $$\n",
    "\n",
    "#### Modifying the `_move()`:\n",
    "\n",
    "Movement logic also needs to change. Instead of using fixed directions like UP or LEFT, we now handle **relative directions** based on the snake’s current heading.\n",
    "\n",
    "Imagine the directions as a clock:\n",
    "`[RIGHT, DOWN, LEFT, UP]`\n",
    "\n",
    "If a snake is heading right:\n",
    "  1. A **right** turn = DOWN\n",
    "  2. A **left** turn = UP\n",
    "  3. **Straight** = continue RIGHT\n",
    "\n",
    "Since the model doesn’t choose absolute directions, it outputs a 3-element array:\n",
    " 1. `[1, 0, 0]` = go straight\n",
    " 2. `[0, 1, 0]` = go right\n",
    " 3. `[0, 0, 1]` = go left\n",
    "\n",
    " We translate it into a new direction like so:\n",
    "\n",
    "```python\n",
    "# [[1, 0, 0], [0, 1, 0], [0, 0, 1]] = [straight, right, left]\n",
    "\n",
    "        clock_wise = [Direction.RIGHT, Direction.DOWN, Direction.LEFT, Direction.UP]\n",
    "        index = clock_wise.index(self.direction)\n",
    "\n",
    "        if np.array_equal(action, [1, 0, 0]):\n",
    "            new_direction = clock_wise[index] # straight - no change\n",
    "        elif np.array_equal(action, [0, 1, 0]):\n",
    "            next_index = (index + 1) % 4\n",
    "            new_direction = clock_wise[next_index] # turn right\n",
    "        else: # [0, 0, 1] = left\n",
    "            next_index = (index - 1) % 4\n",
    "            new_direction = clock_wise[next_index] # turn left\n",
    "\n",
    "        self.direction = new_direction\n",
    "```\n",
    "\n",
    "This logic ensures that the snake moves relative to its current direction — just like how a real animal (or robot) might navigate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "pygame.init()\n",
    "font = pygame.font.Font('arial.ttf', 25)\n",
    "\n",
    "class Direction(Enum):\n",
    "    RIGHT = 1\n",
    "    LEFT = 2\n",
    "    UP = 3\n",
    "    DOWN = 4\n",
    "    \n",
    "Point = namedtuple('Point', 'x, y')\n",
    "\n",
    "# rgb colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (200,0,0)\n",
    "BLUE1 = (0, 0, 255)\n",
    "BLUE2 = (0, 100, 255)\n",
    "BLACK = (0,0,0)\n",
    "\n",
    "BLOCK_SIZE = 20\n",
    "SPEED = 40\n",
    "\n",
    "class SnakeGameAI:\n",
    "    \n",
    "    def __init__(self, w=640, h=480):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        # init display\n",
    "        self.display = pygame.display.set_mode((self.w, self.h))\n",
    "        pygame.display.set_caption('Snake')\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # init game state\n",
    "        self.direction = Direction.RIGHT\n",
    "        \n",
    "        self.head = Point(self.w/2, self.h/2)\n",
    "        self.snake = [self.head, \n",
    "                      Point(self.head.x-BLOCK_SIZE, self.head.y),\n",
    "                      Point(self.head.x-(2*BLOCK_SIZE), self.head.y)]\n",
    "        \n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self._place_food()\n",
    "        self.frame_iteration = 0\n",
    "        \n",
    "    def _place_food(self):\n",
    "        x = random.randint(0, (self.w-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE \n",
    "        y = random.randint(0, (self.h-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n",
    "        self.food = Point(x, y)\n",
    "        if self.food in self.snake:\n",
    "            self._place_food()\n",
    "        \n",
    "    def play_step(self, action):\n",
    "        self.frame_iteration += 1\n",
    "        # 1. collect user input\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "        \n",
    "        # Store previous distance to food (for directional reward)\n",
    "        prev_dist = abs(self.head.x - self.food.x) + abs(self.head.y - self.food.y)\n",
    "        \n",
    "        # 2. move\n",
    "        self._move(action) # update the head\n",
    "        self.snake.insert(0, self.head)\n",
    "        \n",
    "        # 3. check if game over\n",
    "        reward = 0\n",
    "        game_over = False\n",
    "        # Prevents endless loops (bigger snake more moves allowed)\n",
    "        if self.is_collision() or self.frame_iteration > 100*len(self.snake):\n",
    "            game_over = True\n",
    "            reward = -10\n",
    "            return reward, game_over, self.score\n",
    "        \n",
    "        nearby_body = 0\n",
    "        directions = [\n",
    "            (-20, 0), (20, 0), (0, -20), (0, 20),  # Adjacent\n",
    "            (-20, -20), (20, -20), (-20, 20), (20, 20)  # Diagonals\n",
    "        ]\n",
    "\n",
    "        for dx, dy in directions:\n",
    "            if Point(self.head.x + dx, self.head.y + dy) in self.snake[2:]:\n",
    "                nearby_body += 1\n",
    "            \n",
    "        # 4. place new food or just move\n",
    "        if self.head == self.food:\n",
    "            self.score += 1\n",
    "            reward = 10\n",
    "            self._place_food()\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "\n",
    "        # Directional rewards\n",
    "        new_dist = abs(self.head.x - self.food.x) + abs(self.head.y - self.food.y)\n",
    "        reward += 1.0 if new_dist < prev_dist else -0.5\n",
    "        reward += -0.1 * nearby_body\n",
    "        \n",
    "        print(f\"Move: {'Straight' if action[0] else 'Right' if action[1] else 'Left'} | \" \n",
    "            f\"Dist: {prev_dist:.1f}→{new_dist:.1f} | \"\n",
    "            f\"Body blocks: {nearby_body} | Reward: {reward:.1f}\")\n",
    "        \n",
    "        # 5. update ui and clock\n",
    "        self._update_ui()\n",
    "        self.clock.tick(SPEED)\n",
    "        # 6. return game over and score\n",
    "        return reward, game_over, self.score\n",
    "    \n",
    "    def is_collision(self, point = None):\n",
    "        if point is None:\n",
    "            point = self.head\n",
    "        # hits boundary\n",
    "        if point.x > self.w - BLOCK_SIZE or point.x < 0 or point.y > self.h - BLOCK_SIZE or point.y < 0:\n",
    "            return True\n",
    "        # hits itself\n",
    "        if point in self.snake[1:]:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    def _update_ui(self):\n",
    "        self.display.fill(BLACK)\n",
    "        \n",
    "        for pt in self.snake:\n",
    "            pygame.draw.rect(self.display, BLUE1, pygame.Rect(pt.x, pt.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "            pygame.draw.rect(self.display, BLUE2, pygame.Rect(pt.x+4, pt.y+4, 12, 12))\n",
    "            \n",
    "        pygame.draw.rect(self.display, RED, pygame.Rect(self.food.x, self.food.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "        \n",
    "        text = font.render(\"Score: \" + str(self.score), True, WHITE)\n",
    "        self.display.blit(text, [0, 0])\n",
    "        pygame.display.flip()\n",
    "        \n",
    "    def _move(self, action):\n",
    "        # [[1, 0, 0], [0, 1, 0], [0, 0, 1]] = [straight, right, left]\n",
    "\n",
    "        clock_wise = [Direction.RIGHT, Direction.DOWN, Direction.LEFT, Direction.UP]\n",
    "        index = clock_wise.index(self.direction)\n",
    "\n",
    "        if np.array_equal(action, [1, 0, 0]):\n",
    "            new_direction = clock_wise[index] # straight - no change\n",
    "        elif np.array_equal(action, [0, 1, 0]):\n",
    "            next_index = (index + 1) % 4\n",
    "            new_direction = clock_wise[next_index] # turn right\n",
    "        else: # [0, 0, 1] = left\n",
    "            next_index = (index - 1) % 4\n",
    "            new_direction = clock_wise[next_index] # turn left\n",
    "\n",
    "        self.direction = new_direction\n",
    "\n",
    "        x = self.head.x\n",
    "        y = self.head.y\n",
    "        if self.direction == Direction.RIGHT:\n",
    "            x += BLOCK_SIZE\n",
    "        elif self.direction == Direction.LEFT:\n",
    "            x -= BLOCK_SIZE\n",
    "        elif self.direction == Direction.DOWN:\n",
    "            y += BLOCK_SIZE\n",
    "        elif self.direction == Direction.UP:\n",
    "            y -= BLOCK_SIZE\n",
    "            \n",
    "        self.head = Point(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8e75a",
   "metadata": {},
   "source": [
    "### Next: we need to build the model\n",
    "\n",
    "For the model we will be **extending Reinforcement Learning** by using a **Deep Neural Network** to predict the actions.\n",
    "\n",
    "In the `model.py` we have **2** classes. The `Linear_QNet` and the `QTrainer`.\n",
    "\n",
    "**Linear_QNet**:\n",
    "1. `__init__()` - We initialize the **NN** structure\n",
    "2. `forward()` - Calculate the output result\n",
    "3. `save()` - Save the model\n",
    "4. `load()` - Load the model\n",
    "\n",
    "**QTrainer**:\n",
    "1. `__init__()` - Injects the model, parameters and other dependencies\n",
    "2. `train_step()` - Trains the **NN**\n",
    "\n",
    "So... let's dive a little more.\n",
    "\n",
    "What does the `Linear_QNet` actually do?\n",
    "\n",
    "The model has 3 layers. The Input layer, which is the state of the game (19 values), a hidden layer of 255 neurons (to help the model learn more complex patterns), and the output layer (the 3 possible actions - straight, right, left).\n",
    "\n",
    "The `forward()` function takes in the state and returns 3 **Q-values** [0.3, -0.2, 1.1] - How good each action is in this situation. In this case it picks turning left as the best possibility.\n",
    "\n",
    "Now, what does the `QTrainer` do?\n",
    "\n",
    "The Trainer teaches the model by feeding in experiences and letting it adjust it's weights.\n",
    "\n",
    "An **experience** is: STATE → took ACTION → got REWARD → ended up in NEXT_STATE\n",
    "\n",
    "First, we turn everything into _tensors_, which is the data type torch works with.\n",
    "```python\n",
    "state = torch.tensor(state, dtype=torch.float)\n",
    "```\n",
    "\n",
    "We add a batch dimension if we have only one sample, becayse PyTorch expects batches. Batching also allows training on multiple experiences at once.\n",
    "```python\n",
    "if len(state.shape) == 1:\n",
    "    state = torch.unsqueeze(state, 0)\n",
    "    ...\n",
    "```\n",
    "\n",
    "Then we predict the Q-values for the current state\n",
    "```python\n",
    "pred = self.model(state)\n",
    "```\n",
    "\n",
    "Which then gives Q-values for all actions `[0.2, 0.1, -0.3]`\n",
    "\n",
    "We clone it to make a target, where we will adjust the right action's values to show the model what it should have predicted\n",
    "```python\n",
    "target = pred.clone()\n",
    "```\n",
    "\n",
    "Calculate the target Q-value using a formula derived from the _Bellman Equation_ $ Q_new = reward + \\gamma * max(Q(next_state)) $\n",
    "\n",
    "Gamma (y) is a discount factor that balanced immediate vs future rewards.\n",
    "```python\n",
    "for index in range(len(done)):\n",
    "    Q_new = reward[index]\n",
    "    if not done[index]:\n",
    "        Q_new = reward[index] + gamma * max(self.model(next_state[index]))\n",
    "```\n",
    "\n",
    "The formula means: The value of this action = what I got now + how good things might be if I keep going.\n",
    "\n",
    "Then we replace only the action taken:\n",
    "```python\n",
    "target[index][torch.argmax(action).item()] = Q_new\n",
    "```\n",
    "\n",
    "Let's say the action taken was **2**. Then we update:\n",
    "```python\n",
    "target[index][2] = Q_new\n",
    "```\n",
    "\n",
    "So now:\n",
    "```python\n",
    "target = [0.2, 0.2, Q_new]\n",
    "```\n",
    "\n",
    "This is what we want the model to learn to predict next time.\n",
    "\n",
    "And now we train it!\n",
    "```python\n",
    "self.optimizer.zero_grad()\n",
    "loss = self.criterion(target, pred)\n",
    "loss.backward()\n",
    "\n",
    "self.optimizer.step()\n",
    "```\n",
    "\n",
    "And lastly we feed all that into the model, calculate the loss, compute gradients and update it.\n",
    "\n",
    "`.zero_grad()`: clears out previous gradients\n",
    "\n",
    "`loss`: difference between what the model predicted and what it _should_ predict\n",
    "\n",
    "`.backward()`: computes gradients using gradient descent\n",
    "\n",
    "`.step()`: updates the model using those gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        # Two hidden layers for better pattern recognition\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # ReLU(x) = max(0, x) -> activation function\n",
    "        state = F.relu(self.linear1(state))\n",
    "        state = F.relu(self.linear2(state))\n",
    "        state = self.linear3(state)\n",
    "        return state\n",
    "    \n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = '.\\model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)\n",
    "\n",
    "    def load(self, file_name='model.pth'):\n",
    "        model_folder_path = '.\\model'\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "\n",
    "        if os.path.exists(file_name):\n",
    "            self.load_state_dict(torch.load(file_name))\n",
    "            self.eval()  # Set the model to evaluation mode\n",
    "            print(f'Model loaded from {file_name}')\n",
    "            return True\n",
    "        else:\n",
    "            print(f'No saved model found at {file_name}')\n",
    "            return False\n",
    "\n",
    "\n",
    "class QTrainer:\n",
    "    def __init__(self, model, learning_rate, gamma):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        action = torch.tensor(action, dtype=torch.long)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "\n",
    "        # Checks shape of the state, It's 1 if it's a single state and 2 if it's a batch of states\n",
    "        # If it's a single state, we need to add a batch dimension at the beginning\n",
    "        if len(state.shape) == 1:\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            done = (done, )\n",
    "\n",
    "        # 1: predicted Q values with current state\n",
    "        pred = self.model(state)\n",
    "\n",
    "        target = pred.clone()\n",
    "        for index in range(len(done)): # Iterating over all games (all inputs have the same size/length)\n",
    "            Q_new = reward[index]\n",
    "            if not done[index]:\n",
    "                # Q_new = r + y * max(next_predicted Q value) -> only do this if not done\n",
    "                Q_new = reward[index] + self.gamma * torch.max(self.model(next_state[index]))\n",
    "\n",
    "            # target[batch_index][max Q value index] = Q_new | argmax returns the index of the max Q value\n",
    "            target[index][torch.argmax(action).item()] = Q_new \n",
    "\n",
    "        self.optimizer.zero_grad() # clear previous gradients\n",
    "        loss = self.criterion(target, pred)\n",
    "        loss.backward() # backpropagation\n",
    "\n",
    "        self.optimizer.step() # update weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb3412",
   "metadata": {},
   "source": [
    "### The Agent\n",
    "\n",
    "Now we need to connect everything, we do that by creating an Agent `agent.py`\n",
    "\n",
    "**Agent**:\n",
    "1. `__init__(load_model=False, continue_training=False)` - Initialize variables, the memory, the model, the trainer etc.\n",
    "2. `get_state(game: SnakeGameAI)`- Builds the state of the game (snake position, direction, danger etc.)\n",
    "3. `remember(state, action, reward, next_state, done)` - Saves a single experience into memory for later training (long-term memory)\n",
    "4. `train_long_memory()` - Trains the model using a random batch of past experiences\n",
    "5. `train_short_memory(state, action, reward, next_state, done)` - Trains the model on the most recent experience\n",
    "6. `get_action(state)` - Returns the action taken for that state\n",
    "\n",
    "Diving a little deeper in those functions...\n",
    "\n",
    "The `get_state()` builds the state vector in the current frame of the game.\n",
    "\n",
    "It has 19 values:\n",
    "\n",
    "- **Danger**: straight, right, left\n",
    "- **Current direction**: left, right, up, down\n",
    "- **Food direction**: left, right, up, down\n",
    "- **Body proximity** (8 directions): left, right, up, down, top-left, top-right, bottom-left, bottom-right\n",
    "\n",
    "\n",
    "```python\n",
    "state = [\n",
    "            # Danger straight\n",
    "            (dir_r and game.is_collision(point_r)) or\n",
    "            (dir_l and game.is_collision(point_l)) or\n",
    "            (dir_u and game.is_collision(point_u)) or\n",
    "            (dir_d and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger right\n",
    "            (dir_u and game.is_collision(point_r)) or\n",
    "            (dir_d and game.is_collision(point_l)) or\n",
    "            (dir_l and game.is_collision(point_u)) or\n",
    "            (dir_r and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger left\n",
    "            (dir_d and game.is_collision(point_r)) or\n",
    "            (dir_u and game.is_collision(point_l)) or\n",
    "            (dir_r and game.is_collision(point_u)) or\n",
    "            (dir_l and game.is_collision(point_d)),\n",
    "\n",
    "            # Move direction\n",
    "            dir_l,\n",
    "            dir_r,\n",
    "            dir_u,\n",
    "            dir_d,\n",
    "\n",
    "            # Food location\n",
    "            game.food.x < game.head.x, # food left\n",
    "            game.food.x > game.head.x, # food right\n",
    "            game.food.y < game.head.y, # food up\n",
    "            game.food.y > game.head.y  # food down\n",
    "        ]\n",
    "\n",
    "        # Add body proximity detection\n",
    "        head = game.snake[0]\n",
    "        directions = [\n",
    "            (-20, 0),   # left\n",
    "            (20, 0),    # right\n",
    "            (0, -20),   # up\n",
    "            (0, 20),    # down\n",
    "            (-20, -20), # top-left\n",
    "            (20, -20),  # top-right\n",
    "            (-20, 20),  # bottom-left\n",
    "            (20, 20)    # bottom-right\n",
    "        ]\n",
    "\n",
    "        for dx, dy in directions:\n",
    "            point = Point(head.x + dx, head.y + dy)\n",
    "            state.append(True if point in game.snake[2:] else False)\n",
    "```\n",
    "\n",
    "The `remember()` simply stores a single experience in the memory.\n",
    "\n",
    "```python\n",
    "self.memory.append((state, action, reward, next_state, done))\n",
    "```\n",
    "\n",
    "The `train_short_memory()` calls the trainer's `train_step()` function with **One** experience.\n",
    "\n",
    "```python\n",
    "self.trainer.train_step(state, action, reward, next_state, done)\n",
    "```\n",
    "\n",
    "The `train_long_memory()` calls the trainer's `train_step()` function with a batch of experiences.\n",
    "If the memory's experiences outgrow our `BATCH_SIZE` we get a random batch of `BATCH_SIZE` experiences and train on them.\n",
    "\n",
    "**zip(*mini_sample)** unpacks the list into individual tuples.\n",
    "\n",
    "```python\n",
    "if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "```\n",
    "\n",
    "The `get_action()` is interesting. We have an **epsilon** value, which is the randomness of each move. As the model learns through more games, we reduce randomness.\n",
    "\n",
    "```python\n",
    "self.epsilon -= self.number_of_games\n",
    "action = [0, 0, 0]\n",
    "if random.randint(0, 200) < self.epsilon:\n",
    "    move = random.randint(0, 2)\n",
    "    action[move] = 1\n",
    "```\n",
    "\n",
    "When we are not making random moves, this means that the moves are generated by the model's prediction.\n",
    "We get the index of the biggest Q-value and get the move corresponding to the index.\n",
    "\n",
    "```python\n",
    "state0 = torch.tensor(state, dtype=torch.float)\n",
    "prediction = self.model(state0)\n",
    "move = torch.argmax(prediction).item()\n",
    "action[move] = 1\n",
    "```\n",
    "\n",
    "#### **Tying it all together**\n",
    "\n",
    "We define a `train()` function that creates an instance of the **Agent** and the **SnakeGameAI**, and starts the learning loop. In each iteration:\n",
    "\n",
    "1. The agent observes the current state of the game.\n",
    "2. It decides an action (either random or model-predicted).\n",
    "3. The game processes the action and returns the next state and reward.\n",
    "4. The agent trains on this immediate experience (short memory).\n",
    "5. It stores the experience in long-term memory.\n",
    "6. If the game ends (the snake dies), we:\n",
    "    - Reset the game,\n",
    "    - Train on a batch of past experiences (long memory),\n",
    "    - Update and track scores,\n",
    "    - Save the model if it reaches a new high score.\n",
    "\n",
    "Over many iterations, the agent learns from its mistakes and successes, gradually improving its ability to survive and collect food — all without hardcoded game strategies.\n",
    "But training an agent over hundreds or thousands of games generates a lot of data. To monitor progress, spot trends, and verify improvements, we need a way to visualize performance.\n",
    "\n",
    "That's where `helper.py` comes in — a lightweight utility to plot scores and moving averages after each game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ad28c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from game import SnakeGameAI, Direction, Point\n",
    "from model import Linear_QNet, QTrainer\n",
    "from helper import plot\n",
    "\n",
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, load_model=False, continue_training=False):\n",
    "        self.number_of_games = 0\n",
    "        self.epsilon = 80 # control randomness\n",
    "        self.gamma = 0.9 # discount rate\n",
    "        self.memory = deque(maxlen=MAX_MEMORY) # popleft()\n",
    "        self.model = Linear_QNet(19, 256, 3)\n",
    "        self.trainer = QTrainer(self.model, learning_rate=LEARNING_RATE, gamma=self.gamma)\n",
    "\n",
    "        if load_model:\n",
    "            success = self.model.load('model.pth')\n",
    "            if success and continue_training:\n",
    "                self.model.train()  # Set the model to training mode if continuing training\n",
    "                self.epsilon = 50 # Higher than fully trained, lower than new\n",
    "            else:\n",
    "                self.epsilon = 20 # Start with some exploration\n",
    "\n",
    "    def get_state(self, game):\n",
    "        head = game.snake[0]\n",
    "        point_l = Point(head.x - 20, head.y) # we use the const 20 because this is the size of each snake rectangle\n",
    "        point_r = Point(head.x + 20, head.y)\n",
    "        point_u = Point(head.x, head.y - 20)\n",
    "        point_d = Point(head.x, head.y + 20)\n",
    "\n",
    "        dir_l = game.direction == Direction.LEFT\n",
    "        dir_r = game.direction == Direction.RIGHT\n",
    "        dir_u = game.direction == Direction.UP\n",
    "        dir_d = game.direction == Direction.DOWN\n",
    "\n",
    "        # This is a little confusing before it clicks :D\n",
    "        state = [\n",
    "            # Danger straight\n",
    "            (dir_r and game.is_collision(point_r)) or\n",
    "            (dir_l and game.is_collision(point_l)) or\n",
    "            (dir_u and game.is_collision(point_u)) or\n",
    "            (dir_d and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger right\n",
    "            (dir_u and game.is_collision(point_r)) or\n",
    "            (dir_d and game.is_collision(point_l)) or\n",
    "            (dir_l and game.is_collision(point_u)) or\n",
    "            (dir_r and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger left\n",
    "            (dir_d and game.is_collision(point_r)) or\n",
    "            (dir_u and game.is_collision(point_l)) or\n",
    "            (dir_r and game.is_collision(point_u)) or\n",
    "            (dir_l and game.is_collision(point_d)),\n",
    "\n",
    "            # Move direction\n",
    "            dir_l,\n",
    "            dir_r,\n",
    "            dir_u,\n",
    "            dir_d,\n",
    "\n",
    "            # Food location\n",
    "            game.food.x < game.head.x, # food left\n",
    "            game.food.x > game.head.x, # food right\n",
    "            game.food.y < game.head.y, # food up\n",
    "            game.food.y > game.head.y  # food down\n",
    "        ]\n",
    "\n",
    "        # Add body proximity detection\n",
    "        head = game.snake[0]\n",
    "        directions = [\n",
    "            (-20, 0),   # left\n",
    "            (20, 0),    # right\n",
    "            (0, -20),   # up\n",
    "            (0, 20),    # down\n",
    "            (-20, -20), # top-left\n",
    "            (20, -20),  # top-right\n",
    "            (-20, 20),  # bottom-left\n",
    "            (20, 20)    # bottom-right\n",
    "        ]\n",
    "\n",
    "        for dx, dy in directions:\n",
    "            point = Point(head.x + dx, head.y + dy)\n",
    "            state.append(True if point in game.snake[2:] else False)\n",
    "\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # popleft() if MAX_MEMORY is reached\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # random moves: tradeoff exploration vs exploitation\n",
    "        # The more games we play, the less random moves we make\n",
    "        self.epsilon -= self.number_of_games\n",
    "        action = [0, 0, 0]\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            move = random.randint(0, 2)\n",
    "            action[move] = 1\n",
    "        else:\n",
    "            state0 = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state0) # this will call the forward function of the model\n",
    "            move = torch.argmax(prediction).item()\n",
    "            action[move] = 1\n",
    "            \n",
    "        return action\n",
    "\n",
    "def train(load_model=False, continue_training=False):\n",
    "    plot_scores = []\n",
    "    plot_mean_scores = []\n",
    "    total_score = 0 \n",
    "    max_score = 0\n",
    "    agent = Agent(load_model=load_model, continue_training=continue_training)\n",
    "    game = SnakeGameAI()\n",
    "\n",
    "    while True:\n",
    "        # get old state\n",
    "        state_old = agent.get_state(game)\n",
    "\n",
    "        # get action based on state\n",
    "        action = agent.get_action(state_old)\n",
    "\n",
    "        # perform move and get new state\n",
    "        reward, done, score = game.play_step(action)\n",
    "        state_new = agent.get_state(game)\n",
    "\n",
    "        # train short memory\n",
    "        agent.train_short_memory(state_old, action, reward, state_new, done)\n",
    "\n",
    "        # remember\n",
    "        agent.remember(state_old, action, reward, state_new, done)\n",
    "\n",
    "        if done:\n",
    "            # train the long memory, plot result\n",
    "            game.reset()\n",
    "            agent.number_of_games += 1\n",
    "            agent.train_long_memory()\n",
    "\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                agent.model.save()\n",
    "\n",
    "            print('Game', agent.number_of_games, 'Score', score, 'Record', max_score)\n",
    "\n",
    "            plot_scores.append(score)\n",
    "            total_score += score\n",
    "            mean_score = total_score / agent.number_of_games\n",
    "            plot_mean_scores.append(mean_score)\n",
    "            plot(plot_scores, plot_mean_scores) # plot the scores\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfdf79a",
   "metadata": {},
   "source": [
    "#### The plotting\n",
    "\n",
    "As the agent trains, it's important to visualize its progress to understand how well it's learning over time.\n",
    "\n",
    "We use a single helper function: `plot(scores, mean_scores)`.\n",
    "This function updates a live plot showing:\n",
    "    - The raw score after each game\n",
    "    - The average score across all games (mean = total score / number of games)\n",
    "\n",
    "To enable real-time updates without interrupting the training loop, we use:\n",
    "\n",
    "```python\n",
    "# Enable interactive mode\n",
    "plt.ion()\n",
    "```\n",
    "\n",
    "This allows the plot to refresh after each game without blocking the script. It's a simple but powerful way to monitor training and see whether the agent is actually improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "plt.ion() # To plot interactively\n",
    "\n",
    "def plot(scores, mean_scores):\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf()) # Get current figure\n",
    "    plt.clf() # Clear the current figure\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Number of games')\n",
    "    plt.ylabel('Score')\n",
    "    plt.plot(scores)\n",
    "    plt.plot(mean_scores)\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.text(len(scores) - 1, scores[-1], str(scores[-1]))\n",
    "    plt.text(len(mean_scores) - 1, mean_scores[-1], str(mean_scores[-1]))\n",
    "    plt.show(block=False)\n",
    "    plt.pause(0.1) # Pause to update the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73856961",
   "metadata": {},
   "source": [
    "To import an already trained model, we go to `agent.py` -> `train()`, and we change the **load_model** and **continue_training** to **True**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c949bc1",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "https://www.youtube.com/watch?v=PJl4iabBEz0&list=PLfR10wejCzo_OL-6OsBV-4jAPnSncvZZH\n",
    "\n",
    "https://en.wikipedia.org/wiki/Reinforcement_learning\n",
    "\n",
    "https://www.geeksforgeeks.org/what-is-reinforcement-learning/\n",
    "\n",
    "https://www.geeksforgeeks.org/snake-game-in-python-using-pygame-module/\n",
    "\n",
    "https://pytorch.org/\n",
    "\n",
    "https://www.pygame.org/docs/\n",
    "\n",
    "https://docs.pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "\n",
    "\n",
    "And of course... **ChatGPT**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNAKE_GAME_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
